
@article{bach_snorkel_2018,
	title = {Snorkel {DryBell}: A Case Study in Deploying Weak Supervision at Industrial Scale},
	url = {http://arxiv.org/abs/1812.00417},
	shorttitle = {Snorkel {DryBell}},
	abstract = {Labeling training data is one of the most costly bottlenecks in developing or modifying machine learning-based applications. We survey how resources from across an organization can be used as weak supervision sources for three classification tasks at Google, in order to bring development time and cost down by an order of magnitude. We build on the Snorkel framework, extending it as a new system, Snorkel {DryBell}, which integrates with Google's distributed production systems and enables engineers to develop and execute weak supervision strategies over millions of examples in less than thirty minutes. We find that Snorkel {DryBell} creates classifiers of comparable quality to ones trained using up to tens of thousands of hand-labeled examples, in part by leveraging organizational resources not servable in production which contribute an average 52\% performance improvement to the weakly supervised classifiers.},
	journaltitle = {{arXiv}:1812.00417 [cs, stat]},
	author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alexander and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and Ré, Christopher and Malkin, Rob},
	urldate = {2019-03-21},
	date = {2018-12-02},
	eprinttype = {arxiv},
	eprint = {1812.00417},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2018_Snorkel DryBell.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2018_Snorkel DryBell.pdf:application/pdf;arXiv.org Snapshot:/Users/kmamykin/Zotero/storage/R5EPVQBM/1812.html:text/html}
}

@article{bach_learning_2017,
	title = {Learning the Structure of Generative Models without Labeled Data},
	url = {http://arxiv.org/abs/1703.00854},
	abstract = {Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the \${\textbackslash}ell\_1\$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100\${\textbackslash}times\$ faster than a maximum likelihood approach and selects \$1/4\$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as {PubMed} journal abstracts.},
	journaltitle = {{arXiv}:1703.00854 [cs, stat]},
	author = {Bach, Stephen H. and He, Bryan and Ratner, Alexander and Ré, Christopher},
	urldate = {2019-03-21},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1703.00854},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2017_Learning the Structure of Generative Models without Labeled Data.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2017_Learning the Structure of Generative Models without Labeled Data.pdf:application/pdf;arXiv.org Snapshot:/Users/kmamykin/Zotero/storage/8R5FKVJ4/1703.html:text/html}
}

@article{lee_pseudo-label_nodate,
	title = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
	abstract = {We propose the simple and eﬃcient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in eﬀect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the {MNIST} handwritten digit dataset.},
	pages = {6},
	author = {Lee, Dong-Hyun},
	langid = {english},
	file = {Pseudo-Label.pdf:/Users/kmamykin/Zotero/storage/7E92GDGG/Pseudo-Label.pdf:application/pdf}
}

@article{li_survey_2015,
	title = {A Survey on Truth Discovery},
	url = {http://arxiv.org/abs/1505.02463},
	abstract = {Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.},
	journaltitle = {{arXiv}:1505.02463 [cs]},
	author = {Li, Yaliang and Gao, Jing and Meng, Chuishi and Li, Qi and Su, Lu and Zhao, Bo and Fan, Wei and Han, Jiawei},
	urldate = {2019-03-21},
	date = {2015-05-10},
	eprinttype = {arxiv},
	eprint = {1505.02463},
	keywords = {Computer Science - Databases, interesting},
	file = {2015_A Survey on Truth Discovery.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2015_A Survey on Truth Discovery.pdf:application/pdf;arXiv.org Snapshot:/Users/kmamykin/Zotero/storage/VQ4R2LEV/1505.html:text/html}
}

@article{rahm_data_nodate,
	title = {Data Cleaning: Problems and Current Approaches},
	abstract = {We classify data quality problems that are addressed by data cleaning and provide an overview of the main solution approaches. Data cleaning is especially required when integrating heterogeneous data sources and should be addressed together with schema-related data transformations. In data warehouses, data cleaning is a major part of the so-called {ETL} process. We also discuss current tool support for data cleaning.},
	pages = {11},
	author = {Rahm, Erhard and Do, Hong Hai},
	langid = {english},
	file = {Data Cleaning.pdf:/Users/kmamykin/Dropbox/Zotero/storage/Data Cleaning.pdf:application/pdf}
}

@article{rekatsinas_holoclean:_2017,
	title = {{HoloClean}: Holistic Data Repairs with Probabilistic Inference},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3137628.3137631},
	doi = {10.14778/3137628.3137631},
	shorttitle = {{HoloClean}},
	abstract = {We introduce {HoloClean}, a framework for holistic data repairing driven by probabilistic inference. {HoloClean} unifies qualitative data repairing, which relies on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, {HoloClean} automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over {HoloClean}'s probabilistic model scales to instances with millions of tuples. We show that {HoloClean} finds data repairs with an average precision of ∼ 90\% and an average recall of above ∼ 76\% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2× against state-of-the-art methods.},
	pages = {1190--1201},
	number = {11},
	journaltitle = {Proc. {VLDB} Endow.},
	author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F. and Ré, Christopher},
	urldate = {2019-03-21},
	date = {2017-08},
	file = {2017_HoloClean.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2017_HoloClean.pdf:application/pdf}
}

@article{sun_revisiting_2017,
	title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
	url = {http://arxiv.org/abs/1707.02968},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of {GPUs}. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the {JFT}-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	journaltitle = {{arXiv}:1707.02968 [cs]},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	urldate = {2019-03-21},
	date = {2017-07-10},
	eprinttype = {arxiv},
	eprint = {1707.02968},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {2017_Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2017_Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.pdf:application/pdf;arXiv.org Snapshot:/Users/kmamykin/Zotero/storage/L6D5SJ8C/1707.html:text/html}
}

@article{zhang_extracting_2016,
	title = {Extracting Databases from Dark Data with {DeepDive}},
	volume = {2016},
	issn = {0730-8078},
	doi = {10.1145/2882903.2904442},
	abstract = {{DeepDive} is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data - scientific papers, Web classified ads, customer service notes, and so on - were instead in a relational database, it would give analysts a massive and valuable new set of "big data." {DeepDive} is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used {DeepDive} to create databases with accuracy that meets that of human annotators. To date we have successfully deployed {DeepDive} to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by {DeepDive} represents a massive opportunity for industry, government, and scientific researchers. {DeepDive} is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.},
	pages = {847--859},
	journaltitle = {Proceedings. {ACM}-Sigmod International Conference on Management of Data},
	shortjournal = {Proc {ACM} {SIGMOD} Int Conf Manag Data},
	author = {Zhang, Ce and Shin, Jaeho and Ré, Christopher and Cafarella, Michael and Niu, Feng},
	date = {2016-07},
	pmid = {28316365},
	pmcid = {PMC5350112},
	file = {2016_Extracting Databases from Dark Data with DeepDive.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2016_Extracting Databases from Dark Data with DeepDive.pdf:application/pdf}
}

@inproceedings{ratner_role_2019,
	title = {The Role of Massively Multi-Task and Weak Supervision in Software 2.0},
	abstract = {Over the last several years, machine learning models have reached new levels of empirical performance across a broad range of domains. Driven both by accuracy improvements and deployment advantages, many organizations have begun to shift to learningcentered software stacks—a newmode that has been called Software 2.0. This approach holds the promise of radically accelerating the construction, maintenance, and deployment of software systems, and opens up a broad research agenda around changes to hardware, systems, and interaction models. However, these approaches require one critical and often prohibitively expensive ingredient: labeled training data. We outline a vision for a Software 2.0 lifecycle centered around the idea that labeling training data can be the primary interface to Software 2.0 systems. In our envisioned approach, Software 2.0 stacks are programmed using weak supervision—i.e. noisier, programmatically-generated training data—which is specified at various levels of declarative abstraction and precision, and then combined using unsupervised statistical techniques. The codebase for Software 2.0 is also radically different: we envision labels for tens or hundreds of different tasks across an organization combined in a massively multitask central model, leading to amortization of labeling costs and new models of software reuse and development. Finally, we envision Software 2.0 stacks deployed by using collected training labels to supervise commodity model architectures over different servable feature sets. We outline the components of this lifecycle, and provide an interim report on Snorkel, our prototype Software 2.0 system, based on our experiences working on problems ranging from ad fraud to medical diagnostics with some of the world’s largest organizations. {ACM} Reference Format: Alexander Ratner, Braden Hancock, and Christopher Ré. 2018. The Role of Massively Multi-Task and Weak Supervision in Software 2.0. In Proceedings of {CIDR} ({CIDR}’19). {ACM}, New York, {NY}, {USA}, 8 pages. https://doi.org/10. 1145/nnnnnnn.nnnnnnn},
	booktitle = {{CIDR}},
	author = {Ratner, Alexander J.},
	date = {2019},
	keywords = {Machine learning, {CIDR}, Code reuse, Computer multitasking, Prototype, Ratner's theorems, Software deployment, Software system, Unsupervised learning},
	file = {2019_The Role of Massively Multi-Task and Weak Supervision in Software 2.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2019_The Role of Massively Multi-Task and Weak Supervision in Software 2.pdf:application/pdf}
}

@article{varma_snuba:_2018,
	title = {Snuba: automating weak supervision to label training data},
	volume = {12},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3291264.3308983},
	doi = {10.14778/3291264.3291268},
	shorttitle = {Snuba},
	abstract = {As deep learning models are applied to increasingly diverse problems, a key bottleneck is gathering enough high-quality training labels tailored to each task. Users therefore turn to weak supervision, relying on imperfect sources of labels like pattern matching and user-deﬁned heuristics. Unfortunately, users have to design these sources for each task. This process can be time consuming and expensive: domain experts often perform repetitive steps like guessing optimal numerical thresholds and developing informative text patterns. To address these challenges, we present Snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. Snuba generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data. We develop a statistical measure that guarantees the iterative process will automatically terminate before it degrades training label quality. Snuba automatically generates heuristics in under ﬁve minutes and performs up to 9.74 F1 points better than the best known user-deﬁned heuristics developed over many days. In collaborations with users at research labs, Stanford Hospital, and on open source datasets, Snuba outperforms other automated approaches like semisupervised learning by up to 14.35 F1 points.},
	pages = {223--236},
	number = {3},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	author = {Varma, Paroma and Ré, Christopher},
	urldate = {2019-04-17},
	date = {2018-11-01},
	langid = {english},
	file = {2018_Snuba.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2018_Snuba.pdf:application/pdf}
}

@article{sinha_fast_2018,
	title = {Fast Dawid-Skene: A Fast Vote Aggregation Scheme for Sentiment Classification},
	url = {https://arxiv.org/abs/1803.02781v3},
	shorttitle = {Fast Dawid-Skene},
	abstract = {Many real world problems can now be effectively solved using supervised
machine learning. A major roadblock is often the lack of an adequate quantity
of labeled data for training. A possible solution is to assign the task of
labeling data to a crowd, and then infer the true label using aggregation
methods. A well-known approach for aggregation is the Dawid-Skene ({DS})
algorithm, which is based on the principle of Expectation-Maximization ({EM}). We
propose a new simple, yet effective, {EM}-based algorithm, which can be
interpreted as a `hard' version of {DS}, that allows much faster convergence
while maintaining similar accuracy in aggregation. We show the use of this
algorithm as a quick and effective technique for online, real-time sentiment
annotation. We also prove that our algorithm converges to the estimated labels
at a linear rate. Our experiments on standard datasets show a significant
speedup in time taken for aggregation - upto \${\textbackslash}sim\$8x over Dawid-Skene and
\${\textbackslash}sim\$6x over other fast {EM} methods, at competitive accuracy performance. The
code for the implementation of the algorithms can be found at
https://github.com/{GoodDeeds}/Fast-Dawid-Skene},
	author = {Sinha, Vaibhav B. and Rao, Sukrut and Balasubramanian, Vineeth N.},
	urldate = {2019-04-27},
	date = {2018-03-07},
	langid = {english},
	file = {2018_Fast Dawid-Skene.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2018_Fast Dawid-Skene.pdf:application/pdf;Snapshot:/Users/kmamykin/Zotero/storage/LHFE6926/1803.html:text/html}
}

@article{chegini_interactive_2019,
	title = {Interactive labelling of a multivariate dataset for supervised machine learning using linked visualisations, clustering, and active learning},
	volume = {3},
	issn = {2468-502X},
	url = {http://www.sciencedirect.com/science/article/pii/S2468502X19300178},
	doi = {10.1016/j.visinf.2019.03.002},
	series = {{SI}: Proceedings of {PacificVAST} 2019},
	abstract = {Supervised machine learning techniques require labelled multivariate training datasets. Many approaches address the issue of unlabelled datasets by tightly coupling machine learning algorithms with interactive visualisations. Using appropriate techniques, analysts can play an active role in a highly interactive and iterative machine learning process to label the dataset and create meaningful partitions. While this principle has been implemented either for unsupervised, semi-supervised, or supervised machine learning tasks, the combination of all three methodologies remains challenging. In this paper, a visual analytics approach is presented, combining a variety of machine learning capabilities with four linked visualisation views, all integrated within the {mVis} (multivariate Visualiser) system. The available palette of techniques allows an analyst to perform exploratory data analysis on a multivariate dataset and divide it into meaningful labelled partitions, from which a classifier can be built. In the workflow, the analyst can label interesting patterns or outliers in a semi-supervised process supported by active learning. Once a dataset has been interactively labelled, the analyst can continue the workflow with supervised machine learning to assess to what degree the subsequent classifier has effectively learned the concepts expressed in the labelled training dataset. Using a novel technique called automatic dimension selection, interactions the analyst had with dimensions of the multivariate dataset are used to steer the machine learning algorithms. A real-world football dataset is used to show the utility of {mVis} for a series of analysis and labelling tasks, from initial labelling through iterations of data exploration, clustering, classification, and active learning to refine the named partitions, to finally producing a high-quality labelled training dataset suitable for training a classifier. The tool empowers the analyst with interactive visualisations including scatterplots, parallel coordinates, similarity maps for records, and a new similarity map for partitions.},
	pages = {9--17},
	number = {1},
	journaltitle = {Visual Informatics},
	shortjournal = {Visual Informatics},
	author = {Chegini, Mohammad and Bernard, Jürgen and Berger, Philip and Sourin, Alexei and Andrews, Keith and Schreck, Tobias},
	urldate = {2019-04-28},
	date = {2019-03-01},
	keywords = {Active learning, Classification, Clustering, Labelling, Multivariate data, Visualisation},
	file = {2019_Interactive labelling of a multivariate dataset for supervised machine learning.pdf:/Users/kmamykin/Dropbox/Zotero/storage/2019_Interactive labelling of a multivariate dataset for supervised machine learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/kmamykin/Zotero/storage/26RDNMNB/S2468502X19300178.html:text/html}
}